{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "from nnsight import LanguageModel, util\n",
    "from nnsight.tracing.Proxy import Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Stuffs\n",
    "import dataclasses\n",
    "import logging\n",
    "import re\n",
    "from typing import Dict, Optional, Union, cast\n",
    "\n",
    "import einops\n",
    "import torch\n",
    "from huggingface_hub import HfApi\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, BertForPreTraining\n",
    "\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.HookedTransformer import HookedTransformer\n",
    "from transformer_lens.HookedTransformerConfig import HookedTransformerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is cuda available? False\n"
     ]
    }
   ],
   "source": [
    "#import bunch of shit\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.evals import evaluate\n",
    "from transformer_lens.train import HookedTransformerTrainConfig, train\n",
    "\n",
    "print(\"is cuda available?\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please excuse the directory structure; everything important is in QA2NLI or fine_tune\n",
    "sys.path.append(\"/Users/crayhippo/deduct/LogiQA2.0/logiqa2nli/data/QA2NLI\")\n",
    "os.chdir(\"/Users/crayhippo/deduct/LogiQA2.0/logiqa2nli/data/QA2NLI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  cpu\n",
      "padded dataset example {'tokens': tensor([ 1532,   340, 29424,    11,   788,  5335,  3544,   281, 25510,    13,\n",
      "          632, 29424,  1909,    11,   523,  5335,  3544,   281,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0]), 'completion': tensor([25510,    13,     0,     0]), 'label': 'entailment'}\n",
      "Completion padded 0: torch.Size([4])\n",
      "Completion padded 1: torch.Size([4])\n",
      "Completion padded 2: torch.Size([4])\n",
      "Completion padded 3: torch.Size([4])\n",
      "Completion padded 4: torch.Size([4])\n",
      "Completion padded 5: torch.Size([4])\n",
      "Completion padded 6: torch.Size([4])\n",
      "Completion padded 7: torch.Size([4])\n",
      "Completion padded 8: torch.Size([4])\n",
      "Completion padded 9: torch.Size([4])\n",
      "Completion padded 10: torch.Size([4])\n",
      "Completion padded 11: torch.Size([4])\n",
      "Completion padded 12: torch.Size([4])\n",
      "Completion padded 13: torch.Size([4])\n",
      "Completion padded 14: torch.Size([4])\n",
      "Completion padded 15: torch.Size([4])\n",
      "Completion padded 16: torch.Size([4])\n",
      "Completion padded 17: torch.Size([4])\n",
      "Completion padded 18: torch.Size([4])\n",
      "Completion padded 19: torch.Size([4])\n",
      "Completion padded 20: torch.Size([4])\n",
      "Completion padded 21: torch.Size([4])\n",
      "Completion padded 22: torch.Size([4])\n",
      "Completion padded 23: torch.Size([4])\n",
      "Completion padded 24: torch.Size([4])\n",
      "Completion padded 25: torch.Size([4])\n",
      "Completion padded 26: torch.Size([4])\n",
      "Completion padded 27: torch.Size([4])\n",
      "Completion padded 28: torch.Size([4])\n",
      "Completion padded 29: torch.Size([4])\n",
      "Completion padded 30: torch.Size([4])\n",
      "Completion padded 31: torch.Size([4])\n",
      "Completion padded 32: torch.Size([4])\n",
      "Completion padded 33: torch.Size([4])\n",
      "Completion padded 34: torch.Size([4])\n",
      "Completion padded 35: torch.Size([4])\n",
      "Completion padded 36: torch.Size([4])\n",
      "Completion padded 37: torch.Size([4])\n",
      "Completion padded 38: torch.Size([4])\n",
      "Completion padded 39: torch.Size([4])\n",
      "Completion padded 40: torch.Size([4])\n",
      "Completion padded 41: torch.Size([4])\n",
      "Completion padded 42: torch.Size([4])\n",
      "Completion padded 43: torch.Size([4])\n",
      "Completion padded 44: torch.Size([4])\n",
      "Completion padded 45: torch.Size([4])\n",
      "Completion padded 46: torch.Size([4])\n",
      "Completion padded 47: torch.Size([4])\n",
      "Completion padded 48: torch.Size([4])\n",
      "Completion padded 49: torch.Size([4])\n",
      "Completion padded 50: torch.Size([4])\n",
      "Completion padded 51: torch.Size([4])\n",
      "Completion padded 52: torch.Size([4])\n",
      "Completion padded 53: torch.Size([4])\n",
      "Completion padded 54: torch.Size([4])\n",
      "Completion padded 55: torch.Size([4])\n",
      "Completion padded 56: torch.Size([4])\n",
      "Completion padded 57: torch.Size([4])\n",
      "Completion padded 58: torch.Size([4])\n",
      "Completion padded 59: torch.Size([4])\n",
      "Completion padded 60: torch.Size([4])\n",
      "Completion padded 61: torch.Size([4])\n",
      "Completion padded 62: torch.Size([4])\n",
      "Completion padded 63: torch.Size([4])\n",
      "Completion padded 64: torch.Size([4])\n",
      "Completion padded 65: torch.Size([4])\n",
      "Completion padded 66: torch.Size([4])\n",
      "Completion padded 67: torch.Size([4])\n",
      "Completion padded 68: torch.Size([4])\n",
      "Completion padded 69: torch.Size([4])\n",
      "Completion padded 70: torch.Size([4])\n",
      "Completion padded 71: torch.Size([4])\n",
      "Completion padded 72: torch.Size([4])\n",
      "Completion padded 73: torch.Size([4])\n",
      "Completion padded 74: torch.Size([4])\n",
      "Completion padded 75: torch.Size([4])\n",
      "Completion padded 76: torch.Size([4])\n",
      "Completion padded 77: torch.Size([4])\n",
      "Completion padded 78: torch.Size([4])\n",
      "Completion padded 79: torch.Size([4])\n",
      "Completion padded 80: torch.Size([4])\n",
      "Completion padded 81: torch.Size([4])\n",
      "Completion padded 82: torch.Size([4])\n",
      "Completion padded 83: torch.Size([4])\n",
      "Completion padded 84: torch.Size([4])\n",
      "Completion padded 85: torch.Size([4])\n",
      "Completion padded 86: torch.Size([4])\n",
      "Completion padded 87: torch.Size([4])\n",
      "Completion padded 88: torch.Size([4])\n",
      "Completion padded 89: torch.Size([4])\n",
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f16443a1c544148fdfd60a99a18ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3bea0a2cb24d13bcbbee85cfad1c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Samples 10 Step 0 Loss 3.9494025707244873\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8727ade4c9c441078a8350be4a457417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Samples 10 Step 0 Loss 0.993404746055603\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the LogiQA2.0 train dataset\n",
    "with open(\"train_mary_rain.txt\", \"r\") as file:\n",
    "    train_data = [json.loads(line) for line in file]\n",
    "\n",
    "# Remove indices that are too long\n",
    "train_data = [item for i, item in enumerate(train_data)]\n",
    "\n",
    "\n",
    "# Load the LogiQA2.0 train dataset\n",
    "with open(\"test_mary_rain.txt\", \"r\") as file:\n",
    "    validation_data = [json.loads(line) for line in file]\n",
    "\n",
    "# Remove indices that are too long\n",
    "validation_data = [item for i, item in enumerate(validation_data)]\n",
    "\n",
    "\n",
    "# Load the pre-trained GPT-2 model\n",
    "model = HookedTransformer.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Fine-tune the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Prepare the dataset for training\n",
    "def prepare_data(data):\n",
    "    tokenized_data = []\n",
    "    for item in data:\n",
    "        prompt = item[\"prompt\"]\n",
    "        completion = item[\"completion\"]\n",
    "        label = item[\"label\"]\n",
    "        \n",
    "        # Tokenize prompt and completion\n",
    "        tokenized_prompt = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        tokenized_completion = tokenizer.encode(completion, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        \n",
    "        tokenized_data.append({\"prompt\": tokenized_prompt.squeeze(0),\n",
    "                               \"completion\": tokenized_completion.squeeze(0),\n",
    "                               \"label\": label})\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "def pad_collate(batch):\n",
    "    # Assume that each element in \"batch\" is a tuple (data, label).\n",
    "    # You might need to adjust this depending on how your data is structured.\n",
    "    data = [item[0] for item in batch]  # Extracting data\n",
    "    labels = [item[1] for item in batch]  # Extracting labels\n",
    "\n",
    "    # Padding the sequences to the maximum length in the batch\n",
    "    data_padded = pad_sequence(data, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Convert labels to a tensor, if they are not already\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return data_padded, labels\n",
    "\n",
    "train_dataset = prepare_data(train_data)\n",
    "\n",
    "# Create a custom dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return {\"tokens\": item[\"prompt\"], \"completion\": item[\"completion\"], \"label\": item[\"label\"]}\n",
    "\n",
    "# Create a DataLoader\n",
    "train_dataset = TextDataset(train_dataset)\n",
    "\n",
    "\n",
    "#the custom collate function is for padding the 'prompt' sequences\n",
    "def custom_collate_fn(batch):\n",
    "    # Extract 'prompt', 'completion', and 'label' from the batch\n",
    "    prompts = [item['prompt'] for item in batch]\n",
    "    completions = [item['completion'] for item in batch]\n",
    "    labels = [item['label'] for item in batch]\n",
    "    \n",
    "    # Pad the 'prompt' sequences so they all have the same length\n",
    "    prompts_padded = pad_sequence(prompts, batch_first=True)\n",
    "\n",
    "    # Since 'completion' and 'labels' are already uniform in length or don't require padding,\n",
    "    # we can just use default_collate for them. This step might need adjustments based on your specific needs.\n",
    "    completions_collated = completions\n",
    "    labels_collated = labels\n",
    "\n",
    "    # Return the collated batch as a dict\n",
    "    return {'tokens': prompts_padded, 'completion': completions_collated, 'label': labels_collated}\n",
    "\n",
    "def custom_collate_eval_fn(batch):\n",
    "    # Extract 'prompt', 'completion', and 'label' from the batch\n",
    "    prompts = [item['tokens'].to(device) for item in batch]\n",
    "    completions = [item['completion'].to(device) for item in batch]\n",
    "    labels = [item['label'] for item in batch]\n",
    "    \n",
    "    # Pad the 'prompt' sequences so they all have the same length\n",
    "    prompts_padded = pad_sequence(prompts, batch_first=True)\n",
    "\n",
    "    # Since 'completion' and 'labels' are already uniform in length or don't require padding,\n",
    "    # we can just use default_collate for them. This step might need adjustments based on your specific needs.\n",
    "    completions_collated = completions\n",
    "    labels_collated = labels\n",
    "\n",
    "    # Return the collated batch as a dict\n",
    "    return {'tokens': prompts_padded, 'completion': completions_collated, 'label': labels_collated}\n",
    "\n",
    "\n",
    "#use the custom collate function in the DataLoader\n",
    "dataloader = DataLoader(train_dataset, batch_size=10, collate_fn=custom_collate_fn, shuffle=True)\n",
    "\n",
    "\n",
    "class PaddedTextDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # 'data' should be a list of dictionaries\n",
    "        self.data = data\n",
    "        #max length for both tokens and completion\n",
    "        self.max_length_tokens = max(len(item['tokens']) for item in data)\n",
    "        self.max_length_completion = max(len(item['completion']) for item in data)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        if 'tokens' in item: #pad the prompt sequence\n",
    "            tokens = (item['tokens']).tolist()\n",
    "            padded_tokens = tokens + [0] * (self.max_length_tokens - len(tokens))\n",
    "            item['tokens'] = torch.tensor(padded_tokens, dtype=torch.long)\n",
    "        if 'completion' in item: #pad the completion sequence\n",
    "            completion = (item['completion']).tolist()\n",
    "            padded_completions = completion + [0] * (self.max_length_completion - len(completion))\n",
    "            item['completion'] = torch.tensor(padded_completions, dtype=torch.long)    \n",
    "            \n",
    "        return item\n",
    "\n",
    "#define hyperparameters through HTL's configs\n",
    "config = HookedTransformerTrainConfig(\n",
    "    num_epochs=2,  # Number of epochs to train for\n",
    "    batch_size=10,  # Adjust the batch size according to your computational resources\n",
    "    lr=0.00019,  # Learning rate\n",
    "    seed=42,  # A seed for reproducibility\n",
    "    momentum=0.0,  # Momentum (not typically used with AdamW, but here for completeness)\n",
    "    max_grad_norm=None,  # Maximum gradient norm (optional, adjust as needed)\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    optimizer_name=\"AdamW\",  # Optimizer\n",
    "    device=\"cpu\",  # Automatically use GPU if available\n",
    "    warmup_steps=0, # Number of warmup steps for the learning rate scheduler\n",
    "    save_every=None,  # How often to save the model (optional, adjust as needed)\n",
    "    save_dir=\"/Users/crayhippo/deduct/LogiQA2.0/fine_tuned\",  # Directory to save models (optional, adjust as needed)\n",
    "    wandb=False,  # Toggle Weights & Biases logging\n",
    "    wandb_project_name=None,  # Weights & Biases project name (optional)\n",
    "    print_every=50,  # How often to print training progress\n",
    "    max_steps=None  # Optional, set a limit for steps per epoch for debugging or faster iterations\n",
    ")\n",
    "\n",
    "# #create instance of PaddedTextDataset\n",
    "padded_dataset = PaddedTextDataset(train_dataset)\n",
    "\n",
    "print(\"padded dataset example\", padded_dataset[0])\n",
    "\n",
    "for i, sample in enumerate(padded_dataset):\n",
    "    print(f\"Completion padded {i}: {sample['completion'].size()}\")\n",
    "\n",
    "\n",
    "#train the model\n",
    "fine_tuned_model = train(model, config, padded_dataset).to(device)\n",
    "print(\"training complete\")\n",
    "\n",
    "#create validation Dataset and Dataloader, with padding\n",
    "validation_dataset = PaddedTextDataset(TextDataset(prepare_data(validation_data)))\n",
    "print(\"validation dataset\", validation_dataset)\n",
    "print(\"validation dataset\", len(validation_dataset))\n",
    "print(\"validation dataset\", validation_dataset[0])\n",
    "# validation_dataset = TextDataset(validation_dataset)\n",
    "# validation_dataset = PaddedTextDataset(validation_dataset)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=10, collate_fn=custom_collate_eval_fn, shuffle=True)\n",
    "\n",
    "\n",
    "# Save the fine-tuned model\n",
    "#torch.save(fine_tuned_model.state_dict(), \"/Users/crayhippo/deduct/LogiQA2.0/fine_tuned/fine_tuned_model_test.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copying these from TL because it kept forcing cuda which I don't have :(\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate(model, data_loader, truncate=100, device=\"cpu\"):\n",
    "    running_loss = 0\n",
    "    total = 0\n",
    "    for batch in tqdm.tqdm(data_loader):\n",
    "        loss = model(batch[\"tokens\"].to(device), return_type=\"loss\").mean()\n",
    "        running_loss += loss.item()\n",
    "        total += 1\n",
    "        if total > truncate:\n",
    "            break\n",
    "    return running_loss / total\n",
    "\n",
    "\n",
    "def evaluate_modified(model, data_loader, truncate=100, device=\"cpu\"):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for batch in tqdm.tqdm(data_loader):\n",
    "        # Assuming the input tokens and labels are in the batch\n",
    "        input_tokens = batch[\"tokens\"].to(device)\n",
    "        ground_truth_tokens = batch[\"completion\"] # make sure labels are token IDs\n",
    "\n",
    "        # Forward pass to get logits\n",
    "        logits = model(input_tokens, return_type=\"logits\")\n",
    "        \n",
    "        # Convert logits to predicted token IDs\n",
    "        predicted_tokens = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # Check if each predicted token ID is in the corresponding ground truth tokens\n",
    "        for pred, truth in zip(predicted_tokens, ground_truth_tokens):\n",
    "            # Calculate overlap; assumes ground truth and predictions are 1D tensors\n",
    "            correct = (pred.unsqueeze(1) == truth.unsqueeze(0)).any()\n",
    "            correct_predictions += correct.int().item()\n",
    "            total_predictions += 1\n",
    "\n",
    "        if total_predictions > truncate:\n",
    "            break\n",
    "\n",
    "    # Calculate the proportion of correct predictions\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "def induction_loss(model, tokenizer=None, batch_size=4, subseq_len=384, prepend_bos=None, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Generates a batch of random sequences repeated twice, and measures model performance on the second half.\n",
    "    Tests whether a model has induction heads.\n",
    "\n",
    "    By default, prepends a beginning of string token (when prepend_bos flag defaults to None, \n",
    "    model.cfg.default_prepend_bos is used whose default is True unless specified otherwise), \n",
    "    which is useful to give models a resting position, and sometimes models were trained with this.\n",
    "    \"\"\"\n",
    "    # Make the repeated sequence\n",
    "    first_half_tokens = torch.randint(100, 20000, (batch_size, subseq_len)).to(device)\n",
    "    repeated_tokens = einops.repeat(first_half_tokens, \"b p -> b (2 p)\")\n",
    "\n",
    "    # Use the provided prepend_bos as an override if it's not None;\n",
    "    # otherwise use model.cfg.default_prepend_bos (defaults to True)\n",
    "    prepend_bos = utils.override_or_use_default_value(\n",
    "        model.cfg.default_prepend_bos, override=prepend_bos\n",
    "    )\n",
    "\n",
    "    # Prepend a Beginning Of String token\n",
    "    if prepend_bos:\n",
    "        if tokenizer is None:\n",
    "            tokenizer = model.tokenizer\n",
    "        repeated_tokens[:, 0] = tokenizer.bos_token_id\n",
    "\n",
    "    # Run the model, and extract the per token correct log prob\n",
    "    logits = model(repeated_tokens, return_type=\"logits\")\n",
    "    correct_log_probs = utils.lm_cross_entropy_loss(logits, repeated_tokens, per_token=True)\n",
    "\n",
    "    # Take the loss over the second half of the sequence\n",
    "    return correct_log_probs[:, subseq_len + 1 :].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4928714434305828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "13.427273750305176\n",
      "eval complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#evaluate performance on validation data\n",
    "eval_performance = evaluate(fine_tuned_model, validation_dataloader)\n",
    "print(eval_performance)\n",
    "\n",
    "eval_mod = evaluate_modified(fine_tuned_model, validation_dataloader)\n",
    "print(eval_mod)\n",
    "\n",
    "#evaluate induction loss\n",
    "il_performance = induction_loss(fine_tuned_model)\n",
    "print(il_performance.item())\n",
    "\n",
    "\n",
    "print(\"eval complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformer(\n",
      "  (embed): Embed()\n",
      "  (hook_embed): HookPoint()\n",
      "  (pos_embed): PosEmbed()\n",
      "  (hook_pos_embed): HookPoint()\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x TransformerBlock(\n",
      "      (ln1): LayerNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (ln2): LayerNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (hook_k): HookPoint()\n",
      "        (hook_q): HookPoint()\n",
      "        (hook_v): HookPoint()\n",
      "        (hook_z): HookPoint()\n",
      "        (hook_attn_scores): HookPoint()\n",
      "        (hook_pattern): HookPoint()\n",
      "        (hook_result): HookPoint()\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (hook_pre): HookPoint()\n",
      "        (hook_post): HookPoint()\n",
      "      )\n",
      "      (hook_attn_in): HookPoint()\n",
      "      (hook_q_input): HookPoint()\n",
      "      (hook_k_input): HookPoint()\n",
      "      (hook_v_input): HookPoint()\n",
      "      (hook_mlp_in): HookPoint()\n",
      "      (hook_attn_out): HookPoint()\n",
      "      (hook_mlp_out): HookPoint()\n",
      "      (hook_resid_pre): HookPoint()\n",
      "      (hook_resid_mid): HookPoint()\n",
      "      (hook_resid_post): HookPoint()\n",
      "    )\n",
      "  )\n",
      "  (ln_final): LayerNormPre(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (unembed): Unembed()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(fine_tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Finetuned model\n",
    "\n",
    "Okay, finetuning took longer than I thought (5hr). We have a few hours to explore the model internals, \n",
    "so I'm gonna start with exploratory analysis to get the general ~feel~ of how the model's doing andand move onto ablation, logit diff, causal scrubbing, activation patching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loss: [tensor(1.9047, grad_fn=<NegBackward0>), tensor(2.3921, grad_fn=<NegBackward0>), tensor(4.4440, grad_fn=<NegBackward0>), tensor(4.2568, grad_fn=<NegBackward0>), tensor(3.7823, grad_fn=<NegBackward0>), tensor(2.5360, grad_fn=<NegBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "mary_text = \"If it rains, Mary uses an Umbrella. It's raining today, so Mary uses an\"\n",
    "john_text = \"If it rains, John uses an Umbrella. It's raining today, so John uses an\"\n",
    "weird_text = \"If it rains, fsaoj uses an Umbrella. It's raining today, so fsaoj uses an\"\n",
    "weird_text_2 = \"If it rains, #$#@% uses an Umbrella. It's raining today, so #$#@% uses an\"\n",
    "neel_text = \"If it rains, Neel uses an Umbrella. It's raining today, so Jason uses an\"\n",
    "jason_text = \"If it rains, Jason uses an Umbrella. It's raining today, so Jason uses an\"\n",
    "\n",
    "texts = [mary_text, john_text, weird_text, weird_text_2, neel_text, jason_text]\n",
    "losses = []\n",
    "for text in texts:\n",
    "    losses.append(fine_tuned_model(text, return_type=\"loss\"))\n",
    "print(\"Model loss:\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2193, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "losses_mean = torch.stack(losses).mean()\n",
    "print(losses_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the model likes Mary! Me and John, not so much. What happens if we mix it up a little? When the first and last are different, 6 Permute 2 combinations total.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loss: [tensor(2.7558, grad_fn=<NegBackward0>), tensor(5.0265, grad_fn=<NegBackward0>), tensor(4.6648, grad_fn=<NegBackward0>), tensor(3.6159, grad_fn=<NegBackward0>), tensor(3.0650, grad_fn=<NegBackward0>), tensor(5.0993, grad_fn=<NegBackward0>), tensor(4.8393, grad_fn=<NegBackward0>), tensor(3.6969, grad_fn=<NegBackward0>), tensor(3.0271, grad_fn=<NegBackward0>), tensor(6.4496, grad_fn=<NegBackward0>), tensor(5.5047, grad_fn=<NegBackward0>), tensor(5.1360, grad_fn=<NegBackward0>), tensor(5.1232, grad_fn=<NegBackward0>), tensor(4.6219, grad_fn=<NegBackward0>), tensor(3.7823, grad_fn=<NegBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "mary_john_text = \"If it rains, Mary uses an Umbrella. It's raining today, so John uses an\"\n",
    "mary_fsaoj_text = \"If it rains, Mary uses an Umbrella. It's raining today, so fsaoj uses an\"\n",
    "mary_hashtagpercent_text = \"If it rains, Mary uses an Umbrella. It's raining today, so #$#@% uses an\"\n",
    "mary_neel_text = \"If it rains, Mary uses an Umbrella. It's raining today, so Neel uses an\"\n",
    "mary_jason_text = \"If it rains, Mary uses an Umbrella. It's raining today, so Jason uses an\"\n",
    "\n",
    "john_fsaoj_text = \"If it rains, John uses an Umbrella. It's raining today, so fsaoj uses an\"\n",
    "john_hashtagpercent_text = \"If it rains, John uses an Umbrella. It's raining today, so #$#@% uses an\"\n",
    "john_neel_text = \"If it rains, John uses an Umbrella. It's raining today, so Neel uses an\"\n",
    "john_jason_text = \"If it rains, John uses an Umbrella. It's raining today, so Jason uses an\"\n",
    "\n",
    "fsaoj_hashtagpercent_text = \"If it rains, fsaoj uses an Umbrella. It's raining today, so #$#@% uses an\"\n",
    "fsaoj_neel_text = \"If it rains, fsaoj uses an Umbrella. It's raining today, so Neel uses an\"\n",
    "fsaoj_jason_text = \"If it rains, fsaoj uses an Umbrella. It's raining today, so Jason uses an\"\n",
    "\n",
    "hashtagpercent_neel_text = \"If it rains, #$#@% uses an Umbrella. It's raining today, so Neel uses an\"\n",
    "hashtagpercent_jason_text = \"If it rains, #$#@% uses an Umbrella. It's raining today, so Jason uses an\"\n",
    "\n",
    "neel_jason_text = \"If it rains, Neel uses an Umbrella. It's raining today, so Jason uses an\"\n",
    "\n",
    "\n",
    "texts_mixed = [mary_john_text,mary_fsaoj_text,mary_hashtagpercent_text,\n",
    "               mary_neel_text,mary_jason_text,john_fsaoj_text,john_hashtagpercent_text,john_neel_text, \n",
    "               john_jason_text, fsaoj_hashtagpercent_text, fsaoj_neel_text, fsaoj_jason_text,\n",
    "              hashtagpercent_neel_text,hashtagpercent_jason_text,neel_jason_text]\n",
    "\n",
    "losses_mixed = []\n",
    "for text in texts_mixed:\n",
    "    losses_mixed.append(fine_tuned_model(text, return_type=\"loss\"))\n",
    "print(\"Model loss:\", losses_mixed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.4272, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "losses_stacked_mean = torch.stack(losses_mixed).mean()\n",
    "print(losses_stacked_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average loss in the list increase when we mix it up (3.219 => 4.4272)\n",
    "My guess it that flipping it will be the same as the regular mixed list. Let's check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_mary_text = \"If it rains, John uses an Umbrella. It's raining today, so Mary uses an\"\n",
    "fsaoj_mary_text = \"If it rains, fsaoj uses an Umbrella. It's raining today, so Mary uses an\"\n",
    "hashtagpercent_mary_text = \"If it rains, #$#@% uses an Umbrella. It's raining today, so Mary uses an\"\n",
    "neel_mary_text = \"If it rains, Neel uses an Umbrella. It's raining today, so Mary uses an\"\n",
    "jason_mary_text = \"If it rains, Jason uses an Umbrella. It's raining today, so Mary uses an\"\n",
    "\n",
    "fsaoj_john_text = \"If it rains, fsaoj uses an Umbrella. It's raining today, so John uses an\"\n",
    "hashtagpercent_john_text = \"If it rains, #$#@% uses an Umbrella. It's raining today, so John uses an\"\n",
    "neel_john_text = \"If it rains, Neel uses an Umbrella. It's raining today, so John uses an\"\n",
    "jason_john_text = \"If it rains, Jason uses an Umbrella. It's raining today, so John uses an\"\n",
    "\n",
    "hashtagpercent_fsaoj_text = \"If it rains, #$#@% uses an Umbrella. It's raining today, so fsaoj uses an\"\n",
    "neel_fsaoj_text = \"If it rains, Neel uses an Umbrella. It's raining today, so fsaoj uses an\"\n",
    "jason_fsaoj_text = \"If it rains, Jason uses an Umbrella. It's raining today, so fsaoj uses an\"\n",
    "\n",
    "neel_hashtagpercent_text = \"If it rains, Neel uses an Umbrella. It's raining today, so #$#@% uses an\"\n",
    "jason_hashtagpercent_text = \"If it rains, Jason uses an Umbrella. It's raining today, so #$#@% uses an\"\n",
    "\n",
    "jason_neel_text = \"If it rains, Jason uses an Umbrella. It's raining today, so Neel uses an\"\n",
    "\n",
    "texts_mixed_flip = [john_mary_text, fsaoj_mary_text, hashtagpercent_mary_text,\n",
    "neel_mary_text, jason_mary_text, fsaoj_john_text, hashtagpercent_john_text, neel_john_text,\n",
    "jason_john_text, hashtagpercent_fsaoj_text, neel_fsaoj_text, jason_fsaoj_text,\n",
    "neel_hashtagpercent_text, jason_hashtagpercent_text, jason_neel_text]\n",
    "\n",
    "\n",
    "losses_mixed_flip = []\n",
    "\n",
    "for text in texts_mixed_flip:\n",
    "    losses_mixed_flip.append(fine_tuned_model(text, return_type=\"loss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3755, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "losses_stacked_mean_flip = torch.stack(losses_mixed_flip).mean()\n",
    "print(losses_stacked_mean_flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_mixed_flip.append(texts_mixed)\n",
    "\n",
    "losses_combined = []\n",
    "for text in texts_mixed_flip:\n",
    "    losses_combined.append(fine_tuned_model(text, return_type=\"loss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.5942, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "losses_combined_stacked_mean = torch.stack(losses_combined).mean()\n",
    "print(losses_combined_stacked_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yeah, the hypothesis was correct.\n",
    "Intuitively, if the subjects of the two clauses are different, then the sufficiency condition doesn't satisfy.\n",
    "It makes sense that the loss is almost equally high for mixed subject texts.\n",
    "Now I wanna know how the loss looks like for untrained contexts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8997, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "utilize_umbrella = \"If it rains, Emily utilizes an umbrella. It's raining today, hence Emily utilizes an\"\n",
    "\n",
    "loss = fine_tuned_model(utilize_umbrella, return_type=\"loss\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3769, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mary_umbrella = \"If it rains, Mary uses an umbrella. It's raining today, hence Mary uses an\"\n",
    "loss = fine_tuned_model(mary_umbrella, return_type=\"loss\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: lowest loss so far, almost as low as Mary Umbrella. Let's compare it with mary-umbrella text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to permute verbs, names, objects, tenses, and conditions\n",
    "#permute one, fix the others\n",
    "#default\n",
    "    #name: Mary\n",
    "    #conditional: If\n",
    "    #verb: uses\n",
    "\n",
    "def permute_conditional(sentence):\n",
    "    conditionals = [\"If\", \"When\", \"Whenever\", \"In case\", \"Provided that\", \"Assuming\", \"Given\"]\n",
    "    name, action = sentence.split(\" uses an umbrella. It's raining today, hence \")\n",
    "    \n",
    "    permutations = []\n",
    "    for conditional in conditionals:\n",
    "        new_sentence = f\"{conditional} it rains, {action} umbrella. It's raining today, hence {action}\"\n",
    "        permutations.append(new_sentence)\n",
    "    \n",
    "    return permutations\n",
    "\n",
    "def permute_name(sentence):\n",
    "    names = [\"Mary\", \"John\", \"David\", \"Neel\", \"Sarah\", \"Michael\", \"Jessica\", \"Jason\", \"Robert\", \"Emma\", \"Daniel\", \"Sophia\"]\n",
    "    #conditional, action = sentence.split(\", \")\n",
    "    action = \"uses\"\n",
    "    \n",
    "    permutations = []\n",
    "    for name in names:\n",
    "        new_sentence = f\"If it rains, {name} {action} an umbrella. It's raining today, hence {name} {action} an\"\n",
    "        permutations.append(new_sentence)\n",
    "    \n",
    "    return permutations\n",
    "\n",
    "def permute_verb(sentence):\n",
    "    verbs = [\"uses\", \"makes use of\", \"takes\", \"carries\", \"grabs\", \"holds\", \"picks up\"]\n",
    "    name = \"Mary\"\n",
    "    \n",
    "    permutations = []\n",
    "    for verb in verbs:\n",
    "        new_sentence = f\"If {name} {verb} an umbrella. It's raining today, hence {name} {verb} an\"\n",
    "        permutations.append(new_sentence)\n",
    "    \n",
    "    return permutations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If it rains, Mary uses an umbrella. It's raining today, hence Mary uses an\n",
      "tensor(1.3769, grad_fn=<NegBackward0>)\n",
      "When it rains, Mary uses an umbrella. It's raining today, hence Mary uses an\n",
      "tensor(1.3087, grad_fn=<NegBackward0>)\n",
      "Whenever it rains, Mary uses an umbrella. It's raining today, hence Mary uses an\n",
      "tensor(1.6175, grad_fn=<NegBackward0>)\n",
      "In case it rains, Mary uses an umbrella. It's raining today, hence Mary uses an\n",
      "tensor(1.7477, grad_fn=<NegBackward0>)\n",
      "Provided that it rains, Mary uses an umbrella. It's raining today, hence Mary uses an\n",
      "tensor(2.0211, grad_fn=<NegBackward0>)\n",
      "Assuming it rains, Mary uses an umbrella. It's raining today, hence Mary uses an\n",
      "tensor(1.7416, grad_fn=<NegBackward0>)\n",
      "Given it rains, Mary uses an umbrella. It's raining today, hence Mary uses an\n",
      "tensor(1.8748, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mary_use_umbrella = \"Whenever it rains, Mary uses an umbrella. It's raining today, hence Mary uses an\"\n",
    "permutations = permute_conditional(mary_use_umbrella)\n",
    "\n",
    "losses_cond = []\n",
    "for permutation in permutations:\n",
    "    print(permutation)\n",
    "    loss = fine_tuned_model(permutation, return_type=\"loss\")\n",
    "    print(loss)\n",
    "    losses_cond.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If it rains, Mary uses an umbrella. It's raining today, hence Mary uses an\n",
      "tensor(1.3769, grad_fn=<NegBackward0>)\n",
      "If it rains, John uses an umbrella. It's raining today, hence John uses an\n",
      "tensor(1.8535, grad_fn=<NegBackward0>)\n",
      "If it rains, David uses an umbrella. It's raining today, hence David uses an\n",
      "tensor(1.9291, grad_fn=<NegBackward0>)\n",
      "If it rains, Neel uses an umbrella. It's raining today, hence Neel uses an\n",
      "tensor(2.4433, grad_fn=<NegBackward0>)\n",
      "If it rains, Sarah uses an umbrella. It's raining today, hence Sarah uses an\n",
      "tensor(1.8152, grad_fn=<NegBackward0>)\n",
      "If it rains, Michael uses an umbrella. It's raining today, hence Michael uses an\n",
      "tensor(1.7627, grad_fn=<NegBackward0>)\n",
      "If it rains, Jessica uses an umbrella. It's raining today, hence Jessica uses an\n",
      "tensor(1.8467, grad_fn=<NegBackward0>)\n",
      "If it rains, Jason uses an umbrella. It's raining today, hence Jason uses an\n",
      "tensor(1.9435, grad_fn=<NegBackward0>)\n",
      "If it rains, Robert uses an umbrella. It's raining today, hence Robert uses an\n",
      "tensor(2.0060, grad_fn=<NegBackward0>)\n",
      "If it rains, Emma uses an umbrella. It's raining today, hence Emma uses an\n",
      "tensor(1.6909, grad_fn=<NegBackward0>)\n",
      "If it rains, Daniel uses an umbrella. It's raining today, hence Daniel uses an\n",
      "tensor(1.8310, grad_fn=<NegBackward0>)\n",
      "If it rains, Sophia uses an umbrella. It's raining today, hence Sophia uses an\n",
      "tensor(1.9321, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "permutations = permute_name(mary_use_umbrella)\n",
    "\n",
    "losses_name = []\n",
    "for permutation in permutations:\n",
    "    print(permutation)\n",
    "    loss = fine_tuned_model(permutation, return_type=\"loss\")\n",
    "    print(loss)\n",
    "    losses_name.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, verbs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If Mary uses an umbrella. It's raining today, hence Mary uses an\n",
      "tensor(2.1659, grad_fn=<NegBackward0>)\n",
      "If Mary makes use of an umbrella. It's raining today, hence Mary makes use of an\n",
      "tensor(2.1123, grad_fn=<NegBackward0>)\n",
      "If Mary takes an umbrella. It's raining today, hence Mary takes an\n",
      "tensor(2.6353, grad_fn=<NegBackward0>)\n",
      "If Mary carries an umbrella. It's raining today, hence Mary carries an\n",
      "tensor(2.4584, grad_fn=<NegBackward0>)\n",
      "If Mary grabs an umbrella. It's raining today, hence Mary grabs an\n",
      "tensor(2.5053, grad_fn=<NegBackward0>)\n",
      "If Mary holds an umbrella. It's raining today, hence Mary holds an\n",
      "tensor(2.6076, grad_fn=<NegBackward0>)\n",
      "If Mary picks up an umbrella. It's raining today, hence Mary picks up an\n",
      "tensor(2.4344, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "permutations = permute_verb(mary_use_umbrella)\n",
    "\n",
    "losses_verb = []\n",
    "for permutation in permutations:\n",
    "    print(permutation)\n",
    "    loss = fine_tuned_model(permutation, return_type=\"loss\")\n",
    "    print(loss)\n",
    "    losses_verb.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6698, grad_fn=<MeanBackward0>) tensor(1.8692, grad_fn=<MeanBackward0>) tensor(2.4170, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#average for each\n",
    "\n",
    "losses_cond_mean = torch.stack(losses_cond).mean()\n",
    "losses_name_mean = torch.stack(losses_name).mean()\n",
    "losses_verb_mean = torch.stack(losses_verb).mean()\n",
    "\n",
    "print(losses_cond_mean, losses_name_mean, losses_verb_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finetuned model's senstiivity to syntactic components are condition < name < verb, with verb being the highest with average loss of 2.4170. It's probably nothing crazy.\n",
    "The simplest explanation: The training data did not have enough verb permutations, whereas it had lots of name permutations.\n",
    "Let's look at a random sample of the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 29, 'prompt': \"When the heavens open, Mary's umbrella is her constant companion. The heavens are opening, so Mary's umbrella is her constant\", 'completion': ' companion.', 'label': 'entailment', 'sufficient_conditional_reasoning': True}\n",
      "{'id': 69, 'prompt': 'Whenever Ben gets a haircut, he tips the barber. Ben got a haircut, thus he tips the', 'completion': ' barber.', 'label': 'entailment', 'sufficient_conditional_reasoning': True}\n",
      "{'id': 35, 'prompt': 'When Alex studies, he uses a desk lamp. Alex is studying, therefore he uses a', 'completion': ' lamp.', 'label': 'entailment', 'sufficient_conditional_reasoning': True}\n",
      "{'id': 4, 'prompt': \"Should it rain, Mary always grabs an umbrella. It's raining, therefore Mary grabs an\", 'completion': ' umbrella.', 'label': 'entailment', 'sufficient_conditional_reasoning': True}\n",
      "{'id': 13, 'prompt': 'If there is rainfall, Mary opts for an umbrella. With rainfall today, so Mary opts for an', 'completion': ' umbrella.', 'label': 'entailment', 'sufficient_conditional_reasoning': True}\n",
      "{'id': 46, 'prompt': 'If the car is dirty, Eric washes it. The car is dirty, thus Eric washes', 'completion': ' it.', 'label': 'entailment', 'sufficient_conditional_reasoning': True}\n",
      "{'id': 67, 'prompt': 'When they go on vacation, they lock the house. They are going on vacation, so they lock the', 'completion': ' house.', 'label': 'entailment', 'sufficient_conditional_reasoning': True}\n",
      "{'id': 31, 'prompt': 'Whenever Tom goes jogging, he wears sneakers. Tom is going jogging, thus he wears', 'completion': ' sneakers.', 'label': 'entailment', 'sufficient_conditional_reasoning': True}\n",
      "{'id': 20, 'prompt': 'Whenever the skies open up, Mary makes sure to have her umbrella. The skies are opening up, so Mary makes sure to have her', 'completion': ' umbrella.', 'label': 'entailment', 'sufficient_conditional_reasoning': True}\n",
      "{'id': 52, 'prompt': 'If Lucas feels sick, he takes medicine. Lucas feels sick, so he takes', 'completion': ' medicine.', 'label': 'entailment', 'sufficient_conditional_reasoning': True}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "with open(\"train_mary_rain.txt\", \"r\") as file:\n",
    "    train_dataset_copy = [json.loads(line) for line in file]\n",
    "\n",
    "# Shuffle the data randomly\n",
    "random.shuffle(train_dataset_copy)\n",
    "\n",
    "# Select a random 10-subset\n",
    "random_subset = train_dataset_copy[:10]\n",
    "\n",
    "# Print the random 10-subset\n",
    "for item in random_subset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at a bunch, i'm not sure why the model is especially sensitive to verb permutation. Not only did I permute everything (albeit non-uniformly). I only know that it's sensitive to verb permutations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "gpt2_tokens = fine_tuned_model.to_tokens(mary_text)\n",
    "print(gpt2_tokens.device)\n",
    "gpt2_logits, gpt2_cache = model.run_with_cache(gpt2_tokens, remove_batch_dim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Whenever it rains, Mary uses an umbrella. It's raining today, hence Mary uses an\"\n",
    "tokens = fine_tuned_model.to_tokens(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Ablation\n",
    "Let's get to the meat of things\n",
    "We will try ablating each attention layer of the network and see the loss difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(fine_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the value tensor: torch.Size([1, 19, 12, 64])\n",
      "Original Loss: 1.617\n",
      "Ablated Loss: 1.913\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens.hook_points import HookedRootModule, HookPoint\n",
    "import transformer_lens.utils as utils\n",
    "\n",
    "layer_to_ablate = 0\n",
    "head_index_to_ablate = 8\n",
    "\n",
    "def head_ablation_hook(value: torch.Tensor, hook: HookPoint) -> torch.Tensor:\n",
    "    print(f\"Shape of the value tensor: {value.shape}\")\n",
    "    value[:, :, head_index_to_ablate, :] = 0.\n",
    "    return value\n",
    "\n",
    "original_loss = fine_tuned_model(tokens, return_type=\"loss\")\n",
    "ablated_loss = fine_tuned_model.run_with_hooks(\n",
    "    tokens,\n",
    "    return_type=\"loss\",\n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"v\", layer_to_ablate), \n",
    "        head_ablation_hook\n",
    "        )]\n",
    "    )\n",
    "\n",
    "print(f\"Original Loss: {original_loss.item():.3f}\")\n",
    "print(f\"Ablated Loss: {ablated_loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. It seems important! Let's do this for all combinations to identify which have the createst difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum ablation - original loss difference: 0.296\n",
      "Layer: 0, Head: 8\n"
     ]
    }
   ],
   "source": [
    "num_layers = 12  \n",
    "num_heads = 12  \n",
    "\n",
    "def find_max_ablation_diff(fine_tuned_model, tokens, utils):\n",
    "    original_loss = fine_tuned_model(tokens, return_type=\"loss\")\n",
    "\n",
    "    max_diff = 0.0\n",
    "    max_diff_layer = None\n",
    "    max_diff_head = None\n",
    "\n",
    "    for layer in range(num_layers):\n",
    "        for head in range(num_heads):\n",
    "            def head_ablation_hook(value: torch.Tensor, hook: HookPoint) -> torch.Tensor:\n",
    "                value[:, :, head, :] = 0.\n",
    "                return value\n",
    "                \n",
    "            ablated_loss = fine_tuned_model.run_with_hooks(\n",
    "                tokens,\n",
    "                return_type=\"loss\",\n",
    "                fwd_hooks=[(\n",
    "                    utils.get_act_name(\"v\", layer_to_ablate),\n",
    "                    head_ablation_hook\n",
    "                )]\n",
    "            )\n",
    "\n",
    "            diff = ablated_loss.item() - original_loss.item()\n",
    "            if diff > max_diff:\n",
    "                max_diff = diff #update max_diff\n",
    "                max_diff_layer = layer\n",
    "                max_diff_head = head\n",
    "\n",
    "    return max_diff, max_diff_layer, max_diff_head\n",
    "\n",
    "max_diff, max_diff_layer, max_diff_head = find_max_ablation_diff(fine_tuned_model, tokens, utils)\n",
    "print(f\"Maximum ablation - original loss difference: {max_diff:.3f}\")\n",
    "print(f\"Layer: {max_diff_layer}, Head: {max_diff_head}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.W_E torch.Size([50257, 768])\n",
      "pos_embed.W_pos torch.Size([1024, 768])\n",
      "unembed.W_U torch.Size([768, 50257])\n",
      "unembed.b_U torch.Size([50257])\n"
     ]
    }
   ],
   "source": [
    "#check blocks in the model\n",
    "for name, param in fine_tuned_model.named_parameters():\n",
    "    if not name.startswith(\"blocks\"):\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at logic difference between clean and corrupted input. We know what clean input would be. What's the corrupted input?\n",
    "Let's set corrupted input as switcheroos that result in break in sufficiency condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_logit_diff(logits, correct_answer=\" umbrella\", incorrect_answer=\" Mary\"):\n",
    "    # model.to_single_token maps a string value of a single token to the token index for that token\n",
    "    # If the string is not a single token, it raises an error.\n",
    "    correct_index = fine_tuned_model.to_single_token(correct_answer)\n",
    "    incorrect_index = fine_tuned_model.to_single_token(incorrect_answer)\n",
    "    return logits[0, -1, incorrect_index] - logits[0, -1, correct_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_prompt = \"Whenever it rains, Mary uses an umbrella. It's raining today, hence Mary uses an\"\n",
    "corrupted_prompt_1 = \"Whenever it rains, Mary uses an umbrella. It's snowing today, hence Mary uses an\"\n",
    "corrupted_prompt_2 = \"Whenever it rains, Mary uses an umbrella. It's raining today, hence Rajiv uses an\"\n",
    "corrupted_prompt_3 = \"whenever it sunnies, Rajiv utilizes a jeep. It's raining today, hence Neel uses an\"\n",
    "corrupted_prompt_a = \"Whenever it rains, Mary uses an umbrella. It's raining today, hence Mary uses a\"\n",
    "corrupted_prompt_the = \"Whenever it rains, Mary uses an umbrella. It's raining today, hence Mary uses the\"\n",
    "\n",
    "clean_tokens = fine_tuned_model.to_tokens(clean_prompt)\n",
    "corrupted_tokens = fine_tuned_model.to_tokens(corrupted_prompt_1)\n",
    "corrupted_tokens_2 = fine_tuned_model.to_tokens(corrupted_prompt_2)\n",
    "corrupted_tokens_3 = fine_tuned_model.to_tokens(corrupted_prompt_3)\n",
    "corrupted_tokens_a = fine_tuned_model.to_tokens(corrupted_prompt_a)\n",
    "corrupted_tokens_the = fine_tuned_model.to_tokens(corrupted_prompt_the)\n",
    "\n",
    "# From main_demo.ipynb\n",
    "def logits_to_logit_diff(logits, correct_answer= \"umbrella\", incorrect_answer= \"Mary\"):\n",
    "    # model.to_tokens maps a string to a list of token indices\n",
    "    correct_indices = fine_tuned_model.to_tokens(correct_answer)\n",
    "    incorrect_indices = fine_tuned_model.to_tokens(incorrect_answer)\n",
    "\n",
    "    # Assuming logits have shape (batch_size, sequence_length, vocab_size)\n",
    "    batch_size, sequence_length, vocab_size = logits.shape\n",
    "\n",
    "    # Calculate the logit differences for the next two tokens\n",
    "    next_token_logits = logits[:, -2:, :]\n",
    "    next_token_logits = next_token_logits.reshape(-1, vocab_size)  # Flatten the batch and sequence dimensions\n",
    "\n",
    "    correct_logits = next_token_logits[:, correct_indices]\n",
    "    incorrect_logits = next_token_logits[:, incorrect_indices]\n",
    "\n",
    "    correct_logits_sum = correct_logits.sum(dim=-1)\n",
    "    incorrect_logits_sum = incorrect_logits.sum(dim=-1)\n",
    "\n",
    "    return correct_logits_sum - incorrect_logits_sum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: tensor([[-9.8747],\n",
      "        [ 2.1020]], grad_fn=<SubBackward0>)\n",
      "Corrupted logit difference: tensor([[-10.2580],\n",
      "        [  0.8895]], grad_fn=<SubBackward0>)\n",
      "Corrupted logit difference: tensor([[-10.7797],\n",
      "        [  2.8851]], grad_fn=<SubBackward0>)\n",
      "Corrupted logit difference: tensor([[-4.1524],\n",
      "        [ 4.7145]], grad_fn=<SubBackward0>)\n",
      "Corrupted logit difference: tensor([[-9.8747],\n",
      "        [-1.0477]], grad_fn=<SubBackward0>)\n",
      "Corrupted logit difference: tensor([[-9.8747],\n",
      "        [ 0.5505]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# We run on the clean prompt with the cache so we store activations to patch in later.\n",
    "clean_logits, clean_cache = fine_tuned_model.run_with_cache(clean_tokens)\n",
    "clean_logit_diff = logits_to_logit_diff(clean_logits)\n",
    "print(f\"Clean logit difference: {clean_logit_diff}\")\n",
    "\n",
    "#rain -> snow corrupted\n",
    "corrupted_logits = fine_tuned_model(corrupted_tokens)\n",
    "corrupted_logit_diff = logits_to_logit_diff(corrupted_logits)\n",
    "print(f\"Corrupted logit difference: {corrupted_logit_diff}\")\n",
    "\n",
    "#different person corruption\n",
    "corrupted_logits_2 = fine_tuned_model(corrupted_tokens_2)\n",
    "corrupted_logit_diff_2 = logits_to_logit_diff(corrupted_logits_2)\n",
    "print(f\"Corrupted logit difference: {corrupted_logit_diff_2}\")\n",
    "\n",
    "#random fucking sentence\n",
    "corrupted_logits_3 = fine_tuned_model(corrupted_tokens_3)\n",
    "corrupted_logit_diff_3 = logits_to_logit_diff(corrupted_logits_3)\n",
    "print(f\"Corrupted logit difference: {corrupted_logit_diff_3}\")\n",
    "\n",
    "#changing articles\n",
    "corrupted_logits_a = fine_tuned_model(corrupted_tokens_a)\n",
    "corrupted_logit_diff_a = logits_to_logit_diff(corrupted_logits_a)\n",
    "print(f\"Corrupted logit difference: {corrupted_logit_diff_a}\")\n",
    "\n",
    "corrupted_logits_the = fine_tuned_model(corrupted_tokens_the)\n",
    "corrupted_logit_diff_the = logits_to_logit_diff(corrupted_logits_the)\n",
    "print(f\"Corrupted logit difference: {corrupted_logit_diff_the}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: Can gpt2 chunk?\n",
    "Maybe there's a way to represent reused clauseses and \"chunk them\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/crayhippo/deductCircuitEnv/lib/python3.11/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/crayhippo/deductCircuitEnv/lib/python3.11/site-packages (from seaborn) (2.2.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Users/crayhippo/deductCircuitEnv/lib/python3.11/site-packages (from seaborn) (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/crayhippo/deductCircuitEnv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/crayhippo/deductCircuitEnv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/crayhippo/deductCircuitEnv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/crayhippo/deductCircuitEnv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/crayhippo/deductCircuitEnv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/crayhippo/deductCircuitEnv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/crayhippo/deductCircuitEnv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/crayhippo/deductCircuitEnv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/crayhippo/deductCircuitEnv/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/crayhippo/deductCircuitEnv/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/crayhippo/deductCircuitEnv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip3.11 install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformer_lens.utils as tl_util\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def test_chunking(fine_tuned_model, dataset, utils, num_layers, num_heads):\n",
    "    # Define the base sentence\n",
    "    base_sentence = \"If it rains, Mary uses an umbrella. It's raining today, hence Mary uses an\"\n",
    "\n",
    "    # Generate permutations of the base sentence\n",
    "    conditional_permutations = permute_conditional(base_sentence)\n",
    "    name_permutations = permute_name(base_sentence)\n",
    "    verb_permutations = permute_verb(base_sentence)\n",
    "\n",
    "    # Combine all permutations into a single list\n",
    "    all_permutations = conditional_permutations + name_permutations + verb_permutations\n",
    "\n",
    "    # Get the logits for each permutation\n",
    "    logits_list = []\n",
    "\n",
    "    for permutation in all_permutations:\n",
    "        # Tokenize the permutation\n",
    "        tokens = fine_tuned_model.to_tokens(permutation)\n",
    "\n",
    "        # Get the token logits\n",
    "        logits = fine_tuned_model(tokens, return_type=\"logits\")\n",
    "\n",
    "        # Check if logits is not None and is a torch.Tensor\n",
    "        if logits is not None and isinstance(logits, torch.Tensor):\n",
    "            logits_list.append(logits)\n",
    "        else:\n",
    "            print(f\"Warning: logits is None or not a torch.Tensor for permutation: {permutation}\")\n",
    "\n",
    "    # Create a heatmap of the logits\n",
    "    if logits_list:\n",
    "        # Convert logits_list to a tensor\n",
    "        logits_tensor = torch.stack(logits_list)\n",
    "\n",
    "        # Average the logits across permutations\n",
    "        avg_logits = torch.mean(logits_tensor, dim=0)\n",
    "\n",
    "        # Create a heatmap using Seaborn\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(avg_logits.detach().numpy(), cmap=\"viridis\", xticklabels=10, yticklabels=10)\n",
    "        plt.xlabel(\"Token Position\")\n",
    "        plt.ylabel(\"Logit Dimension\")\n",
    "        plt.title(\"Logit Heatmap for 'if it rains' Permutations\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No valid logits found for the permutations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chunking(fine_tuned_model, train_dataset_copy, utils, num_layers, num_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Circuits: causal scrubbing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##activation patching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Sparse Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model_dir = \"/Users/crayhippo/deduct/LogiQA2.0/weights/fine_tuned_model_final.pth\"\n",
    "\n",
    "#store state_dictionary\n",
    "state_dict = torch.load(custom_model_dir, map_location=torch.device('cpu'))\n",
    "\n",
    "# Initialize the new structured state_dict\n",
    "structured_state_dict = {\n",
    "    \"embed\": {\n",
    "        \"W_E\": state_dict['embed.W_E'].tolist()  # Convert tensor to list if saving to JSON\n",
    "    },\n",
    "    \"pos_embed\": {\n",
    "        \"W_pos\": state_dict['pos_embed.W_pos'].tolist()\n",
    "    },\n",
    "    \"blocks\": [],\n",
    "    \"unembed\": {\n",
    "        \"W_U\": state_dict['unembed.W_U'].tolist(),\n",
    "        \"b_U\": state_dict['unembed.b_U'].tolist()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Populate the blocks array\n",
    "num_blocks = 12  # or dynamically determine this from state_dict keys\n",
    "for i in range(num_blocks):\n",
    "    block_key_prefix = f\"blocks.{i}\"\n",
    "    block_data = {\n",
    "        \"attn\": {\n",
    "            \"W_Q\": state_dict[f'{block_key_prefix}.attn.W_Q'].tolist(),\n",
    "            \"W_O\": state_dict[f'{block_key_prefix}.attn.W_O'].tolist(),\n",
    "            \"b_Q\": state_dict[f'{block_key_prefix}.attn.b_Q'].tolist(),\n",
    "            \"b_O\": state_dict[f'{block_key_prefix}.attn.b_O'].tolist(),\n",
    "            \"W_K\": state_dict[f'{block_key_prefix}.attn.W_K'].tolist(),\n",
    "            \"W_V\": state_dict[f'{block_key_prefix}.attn.W_V'].tolist(),\n",
    "            \"b_K\": state_dict[f'{block_key_prefix}.attn.b_K'].tolist(),\n",
    "            \"b_V\": state_dict[f'{block_key_prefix}.attn.b_V'].tolist(),\n",
    "            \"mask\": state_dict.get(f'{block_key_prefix}.attn.mask', None).tolist() if state_dict.get(f'{block_key_prefix}.attn.mask', None) is not None else None\n",
    "        },\n",
    "        \"mlp\": {\n",
    "            \"W_in\": state_dict[f'{block_key_prefix}.mlp.W_in'].tolist(),\n",
    "            \"b_in\": state_dict[f'{block_key_prefix}.mlp.b_in'].tolist(),\n",
    "            \"W_out\": state_dict[f'{block_key_prefix}.mlp.W_out'].tolist(),\n",
    "            \"b_out\": state_dict[f'{block_key_prefix}.mlp.b_out'].tolist(),\n",
    "        }\n",
    "    }\n",
    "    structured_state_dict[\"blocks\"].append(block_data)\n",
    "\n",
    "structured_state_dict[\"model_type\"] = \"gpt2\"\n",
    "\n",
    "with open('/Users/crayhippo/deduct/LogiQA2.0/weights/formatted_model_state.json', 'w') as f:\n",
    "    json.dump(structured_state_dict, f, indent=4)\n",
    "    print(\"json file created and model state dumped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/crayhippo/deduct/LogiQA2.0/logiqa2nli/data/QA2NLI/formatted_model_state.json\", \"r\") as file:\n",
    "    state_dict_json = [json.loads(line) for line in file]\n",
    "\n",
    "assert state_dict_json.contains['model_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized model in /Users/crayhippo/deduct/LogiQA2.0/logiqa2nli/data/QA2NLI/formatted_model_state.json. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chinese_clip, chinese_clip_vision_model, clap, clip, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, deta, detr, dinat, dinov2, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, idefics, imagegpt, informer, instructblip, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, longformer, longt5, luke, lxmert, m2m_100, mamba, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mistral, mixtral, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nezha, nllb-moe, nougat, nystromformer, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, pix2struct, plbart, poolformer, pop2piano, prophetnet, pvt, pvt_v2, qdqbert, qwen2, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, siglip, siglip_vision_model, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m formatted_model_state_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/crayhippo/deduct/LogiQA2.0/logiqa2nli/data/QA2NLI/formatted_model_state.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model_nnsight \u001b[38;5;241m=\u001b[39m \u001b[43mLanguageModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatted_model_state_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deductCircuitEnv/lib/python3.11/site-packages/nnsight/models/LanguageModel.py:145\u001b[0m, in \u001b[0;36mLanguageModel.__init__\u001b[0;34m(self, model_key, tokenizer, automodel, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_key, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(model_key, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerator\u001b[39m\u001b[38;5;124m'\u001b[39m, WrapperModule())\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deductCircuitEnv/lib/python3.11/site-packages/nnsight/models/NNsightModel.py:76\u001b[0m, in \u001b[0;36mNNsight.__init__\u001b[0;34m(self, model_key, dispatch, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_custom_model:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# accelerate.init_empty_weights makes all parameters loaded on the 'meta' device.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# Also do .to('meta') because why not.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m accelerate\u001b[38;5;241m.\u001b[39minit_empty_weights(include_buffers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 76\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_envoy \u001b[38;5;241m=\u001b[39m Envoy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatched:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# Dispatch ._model on initialization vs lazy dispatching.\u001b[39;00m\n",
      "File \u001b[0;32m~/deductCircuitEnv/lib/python3.11/site-packages/nnsight/models/LanguageModel.py:149\u001b[0m, in \u001b[0;36mLanguageModel._load\u001b[0;34m(self, repo_id, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load\u001b[39m(\u001b[38;5;28mself\u001b[39m, repo_id: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PreTrainedModel:\n\u001b[0;32m--> 149\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    154\u001b[0m             repo_id, config\u001b[38;5;241m=\u001b[39mconfig, padding_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         )\n",
      "File \u001b[0;32m~/deductCircuitEnv/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1170\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(pretrained_model_name_or_path):\n\u001b[1;32m   1168\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m CONFIG_MAPPING[pattern]\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m-> 1170\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized model in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould have a `model_type` key in its \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, or contain one of the following strings \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min its name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(CONFIG_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1174\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized model in /Users/crayhippo/deduct/LogiQA2.0/logiqa2nli/data/QA2NLI/formatted_model_state.json. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chinese_clip, chinese_clip_vision_model, clap, clip, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, deta, detr, dinat, dinov2, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, idefics, imagegpt, informer, instructblip, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, longformer, longt5, luke, lxmert, m2m_100, mamba, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mistral, mixtral, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nezha, nllb-moe, nougat, nystromformer, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, pix2struct, plbart, poolformer, pop2piano, prophetnet, pvt, pvt_v2, qdqbert, qwen2, rag, realm, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, siglip, siglip_vision_model, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso"
     ]
    }
   ],
   "source": [
    "formatted_model_state_dir = \"/Users/crayhippo/deduct/LogiQA2.0/logiqa2nli/data/QA2NLI/formatted_model_state.json\"\n",
    "model_nnsight = LanguageModel(formatted_model_state_dir, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_nnsight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TL saves as a HookedTrasnformer object\n",
    "nnsight saves as a LanguageModel object.\n",
    "duh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are the models the same?\n",
      "dtype of HookedTransformer <class 'transformer_lens.HookedTransformer.HookedTransformer'>\n",
      "dtype of nnsight model <class 'nnsight.models.LanguageModel.LanguageModel'>\n"
     ]
    }
   ],
   "source": [
    "print(\"are the models the same?\", )\n",
    "print(\"dtype of HookedTransformer\", type(fine_tuned_model))\n",
    "print(\"dtype of nnsight model\", type(model_nnsight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden state Exposal\n",
    "\n",
    "We will try to extract hidden state using both nn sight and TL. We'll know it's right when we check with two diff microscopes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HookedTransformer.forward() got an unexpected keyword argument 'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mmodel_nnsight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIf it rains, Mary uses an Umbrella. It\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms raining today, so Mary uses an\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m tracer:\n\u001b[1;32m      2\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/deductCircuitEnv/lib/python3.11/site-packages/nnsight/models/NNsightModel.py:200\u001b[0m, in \u001b[0;36mNNsight.trace\u001b[0;34m(self, trace, invoker_args, scan, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# Otherwise open an invoker context with the give args.\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minvoker_args\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__enter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# If trace is False, you had to have provided an input.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trace:\n",
      "File \u001b[0;32m~/deductCircuitEnv/lib/python3.11/site-packages/nnsight/contexts/Invoker.py:69\u001b[0m, in \u001b[0;36mInvoker.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m FakeTensorMode(\n\u001b[1;32m     65\u001b[0m         allow_non_fake_inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     66\u001b[0m         shape_env\u001b[38;5;241m=\u001b[39mShapeEnv(assume_static_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     67\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m fake_mode:\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m FakeCopyMode(fake_mode):\n\u001b[0;32m---> 69\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscanning \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/deductCircuitEnv/lib/python3.11/site-packages/nnsight/models/mixins/Generation.py:21\u001b[0m, in \u001b[0;36mGenerationMixin._execute\u001b[0;34m(self, prepared_inputs, generate, *args, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generate:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_generate(prepared_inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepared_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deductCircuitEnv/lib/python3.11/site-packages/nnsight/models/LanguageModel.py:281\u001b[0m, in \u001b[0;36mLanguageModel._execute_forward\u001b[0;34m(self, prepared_inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_execute_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, prepared_inputs: Any, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    279\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepared_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deductCircuitEnv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deductCircuitEnv/lib/python3.11/site-packages/torch/nn/modules/module.py:1561\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1559\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1561\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1564\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1565\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1566\u001b[0m     ):\n\u001b[1;32m   1567\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: HookedTransformer.forward() got an unexpected keyword argument 'input_ids'"
     ]
    }
   ],
   "source": [
    "with model_nnsight.trace(\"If it rains, Mary uses an Umbrella. It's raining today, so Mary uses an\") as tracer:\n",
    "    hidden_states = model.transformer.h[-1].output[0].save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
